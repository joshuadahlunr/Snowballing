# C.A.T.G.I.R.L — Compiler and Academic Text Gathering Intelligent Research Library

**C.A.T.G.I.R.L** is a Python-based tool designed to filter and analyze Digital Object Identifiers (DOIs) for academic papers related to Intermediate Representation (IR) and compiler technology. It employs a multi-stage, asynchronous pipeline to identify, validate, and prioritize relevant scholarly articles for research purposes.

---

## Features

* **Multi-Stage Filtering**: Sequential filtering through metadata validation, abstract analysis, and full-text content inspection.
* **Asynchronous and Rate-Limited**: Efficiently processes DOIs using asynchronous HTTP requests with built-in rate limiting.
* **Fuzzy Text Matching**: Uses advanced fuzzy matching to detect thematic relevance in abstracts and full texts.
* **Thematic Group Categorization**: Contains comprehensive keyword groups representing IR topics for precise filtering.
* **Robust Error Handling**: Automatic retries, exponential backoff, and detailed logging for network and data errors.
* **Batch Processing**: Handles large DOI datasets, combining and deduplicating sources seamlessly.
* **Configurable Parameters**: Adjustable thresholds and rate limits to tune performance and precision.

---

## Requirements

* **Python**: Version 3.12 or higher recommended
* **Packages**:

  * `aiohttp` — asynchronous HTTP requests
  * `rapidfuzz` — fuzzy string matching
  * `PyPDF2` (for full-text PDF processing, optional)
  * `ratelimit` (optional)
  * `tqdm` (optional, for progress bars)

Install required packages with:

```bash
pip install aiohttp rapidfuzz PyPDF2 ratelimit tqdm
```

---

## Installation

1. Clone the repository:

   ```bash
   git clone https://github.com/yourusername/CATGIRL.git
   cd CATGIRL
   ```
2. Install dependencies (see above).

---

## Directory Structure and Usage

* **Unfiltered\_DOI\_Files/**
  Place raw DOI `.txt` files here, including output from snowballing or manual additions.

* **filtering/unfiltered\_doi\_combind/**
  Stores combined and deduplicated DOI files generated by the combiner step.

* **filtering/doi\_results/**
  Outputs filtered DOI lists and logs:

  * `filtered_by_abstract_dois.txt`: DOIs filtered by abstract relevance.
  * `filtered_by_body_dois.txt`: DOIs filtered after full-text analysis.
  * Logs and processing summaries.

* **Matched\_Body\_Snippets/** (Optional)
  Contains extracted relevant snippets from paper full texts for manual review.

---

## Workflow Overview

### Stage 1

* Run the following programs with corresponding seed paper
* Run 2back.py 
* Run 6forwardfromlist.py
* Add any additional doi's to the `unfiltered_doi_files` folder

### Stage 2: DOI Combination

* Combine multiple raw DOI files into a single deduplicated list.
* Run the combining script to generate `combined_dois.txt`.

### Stage 3: Abstract Filtering

* Use metadata validation (year, document type, publisher).
* Analyze abstracts with thematic keyword matching using fuzzy logic.
* Output filtered DOIs to `filtered_by_abstract_dois.txt`.

### Stage 4: Full-Text (Body) Retrieval and Filtering

* Detect open access status using APIs (Unpaywall, OpenAlex).
* Download and parse full paper texts (PDF/HTML/plain text).
* Apply detailed thematic and section-specific content analysis.
* Save final filtered DOIs and relevant text snippets.

---

## Instructions to Run C.A.T.G.I.R.L

---

### ⚙️ 0. (Optional) Run Snowballing DOI Collection

Use the recursive citation and reference scripts to generate DOI lists from a seed DOI.

```bash
python src/2back6forward.py
```

This script saves a `.txt` file (e.g., `snowball_from_2480743.txt`) containing the collected DOIs into:

```
filtering/unfiltered_doi_files/
```

You can configure:

* `target_doi`
* reference depth
* citation depth
* output filename

---

### 1. Prepare Raw DOI Files

dois `.txt` files from snowballing are already included. Place any additional dois `.txt` files into:
```
filtering/unfiltered_doi_files/
```
dois `.txt` files should be structured with one doi per line:
```
10.1145/174675.178053
10.1007/bf01018828
10.1145/277650.277752
10.1145/358656.358674
10.1145/237721.237729
10.1145/390014.808289
```

---

### 2. Run Abstract Filtering

```bash
python src/filter_by_abstract.py
```

This script:
* This script combines dois `.txt` files.
* Creates a unique set of dois removing duplicates then generates the combined file at:
```
filtering/unfiltered_doi_combind/combined_dois.txt
```

* Reads the combined DOI file
* Performs metadata and abstract analysis
* Writes filtered DOIs to:

```
filtering/doi_results/filtered_by_abstract_dois.txt
```

---

### 4. Run Full-Text Body Filtering

```bash
python src/filter_by_body.py
```

This script:

* Downloads full papers (if available)
* Extracts body text
* Applies deeper content filtering
* If doi is accepted it is recorded in:

```
filtering/doi_results/filtered_by_body_doi's.txt
```

* JSON data for a manual review file including keywords found, and relevance score is in:

```
filtering/processing/content_analysis.json
```

* If PDF or errors occur, the doi is recorded in:

```
filtering/doi_results/doi's_no_pdfs.txt
```

* All errors are recorded in:
```
filtering/processing/body_fetch.log
```

---

### 5. Review Results

* Check filtered DOI lists for papers that passed each stage
* Inspect logs for errors or rejections
* Use manual review files if needed!

---

### 6. Manual review

* Run the generate manual review files In order to rank papers based on `content_analysis.json`

```
src/generate_manual_review_files.py
```
* This should rank the dois by highest relevance scores and provide keywords found
* Manually review dois and accept the best that support your paper
---

## Configuration

* Adjust filtering thresholds, API keys, and rate limits in the configuration section of each script as needed.
* Ensure you respect API rate limits and terms of service for external services used (Crossref, Unpaywall, OpenAlex).

---

## Output Summary

After running the filters, you will get a summary printed to the console showing:

* Total DOIs processed
* Number of DOIs matching the filtering criteria
* Percentage match rate

---

## Error Handling and Logging

* The scripts will automatically retry failed HTTP requests with exponential backoff.
* Detailed logging will help diagnose issues such as rate limiting or network failures.
* The pipeline will skip problematic DOIs without stopping the entire process.

---

## Troubleshooting and Tips

* For large datasets, increase timeouts or reduce concurrency settings.
* Ensure internet connection is stable.
* For PDF parsing issues, check the file format or update dependencies.
* Monitor API usage to avoid quota exhaustion.

---

## Contribution and Support

Email me, if you need any help. 

---

## License

\[Umm Fill in later, but it's intended for anyone to use free of charge]

###  Configuration Options

| Parameter | Default | Description |
|-----------|---------|-------------|
| MIN_BODY_MATCH_PERCENT | 65% | Minimum content match threshold |
| MAX_DOWNLOAD_RETRIES | 3 | Number of retry attempts |
| PDF_TEXT_EXTRACTOR | pdfminer | PDF text extraction library |
| REQUEST_DELAY | 0.11s | Inter-request delay |

### Output Structure

- `DOI_Results/Filtered_Body_DOI.txt`: Final filtered DOIs.
- `filtering/processing/content_analysis.json`: Relevant text extracts.
- `filtering/processing/body_fetch.log`: Processing logs about errors.
- `filtering/doi_results/doi's_no_pdfs.txt`: For dois that API's cant find pdf.
- `filtering/processing/pdfs`: pdf's that are downloaded and searched through.
